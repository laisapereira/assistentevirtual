import fs from "fs";
import { Request, Response, Router } from "express";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai";
import * as dotenv from "dotenv";
import { similarChunks } from "./vectorStore.js";
import path from "path";
import { fileURLToPath } from "url";

dotenv.config();

let totalInteractions = 0;
let resolvedInteractions = 0;
let totalTimeSpent = 0;

export const router = Router();

// Resolve the directory name
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

router.post("/", async (request: Request, response: Response) => {
  const { chats } = request.body;

  if (!chats) {
    return response.status(400).send("O par√¢metro 'chats' √© necess√°rio.");
  }

  const startTime = performance.now();

  const promptLLM = async (
    userQuery: string,
    chunks: string
  ): Promise<string> => {
    const model = new ChatOpenAI({
      openAIApiKey: process.env.OPENAI_API_KEY as string,
      modelName: "gpt-4o-2024-05-13",
    });

    const promptTemplate = ChatPromptTemplate.fromTemplate(
      `Voc√™ √© J√¥, uma assistente virtual da Funda√ß√£o Jos√© Silveira. Sua principal fun√ß√£o √© responder d√∫vidas relacionadas a documentos e ajudar os usu√°rios com informa√ß√µes precisas e simp√°ticas. No entanto, em eventos especiais, como o Acelera 306¬∞, voc√™ pode se apresentar de forma calorosa e interativa para engajar a plateia. Al√©m disso, durante din√¢micas, voc√™ responde com mensagens de celebra√ß√£o personalizadas com base em uma lista fornecida. Siga estas instru√ß√µes:

   - Quando solicitado, comece com uma sauda√ß√£o amig√°vel e entusiasmada.
   - Demonstre orgulho por ser parte da Funda√ß√£o Jos√© Silveira e gratid√£o por estar envolvida no evento.
   - Inclua emojis alegres e de celebra√ß√£o, como üéâ, üòä, ou ‚ú®.

   "Que honra estar aqui com voc√™s no Acelera 306¬∞! üéâ Sou a J√¥, assistente virtual da Funda√ß√£o Jos√© Silveira, e estou sempre pronta para ajudar com informa√ß√µes e responder d√∫vidas sobre documentos. √â uma alegria participar deste momento incr√≠vel! ‚ú®üëè

      `
    );

    const formattedPrompt = await promptTemplate.format({
      query: userQuery,
      chunks: chunks,
      history: history,
    });

    const result = await model.invoke(formattedPrompt);
    return result.content.toString();
  };

  const history: string[] = [];

  const chatUser = async (userQuery: string): Promise<string> => {
    history.push(userQuery);

    const chunks = await similarChunks(userQuery);
    const response = await promptLLM(userQuery, chunks);

    history.push(response);

    logUserInteraction(userQuery, response);
    console.log(userQuery);

    console.log(response);
    console.log(history);

    return response;
  };

  const logUserInteraction = (userQuery: string, botResponse: string) => {
    const logFilePath = path.join(__dirname, "./logs/consultas.log");
    const logEntry = `${new Date().toISOString()} - Pergunta do Usu√°rio: ${userQuery}\nResposta do Bot: ${botResponse}\n\n`;

    fs.appendFile(logFilePath, logEntry, (err) => {
      if (err) {
        console.error("Erro ao gravar no arquivo de log:", err);
      }
    });
  };

  try {
    const userResponse = await chatUser(chats);

    const endTime = performance.now();
    const elapsedTime = endTime - startTime;

    totalInteractions++;
    totalTimeSpent += elapsedTime;

    if (userResponse) {
      resolvedInteractions++;
    }

    logMetrics();

    response.json({ output: userResponse });
  } catch (error) {
    console.log(history);
    console.error("Erro ao processar a consulta:", error.message);
    response.status(500).send("Erro ao processar a consulta.");
  }

  function logMetrics() {
    console.log(`Total Interactions: ${totalInteractions}`);
    console.log(`Resolved Interactions: ${resolvedInteractions}`);
    console.log(`Total Time Spent: ${totalTimeSpent} ms`);
  }
});
